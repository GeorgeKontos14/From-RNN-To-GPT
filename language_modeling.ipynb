{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from src.embedding import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b615f",
   "metadata": {},
   "source": [
    "## Word2Vec: Word Embedding with Continuous Bag of Words and Skipgram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091deaf2",
   "metadata": {},
   "source": [
    "Step 1: Define Corpus and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"a\", \"fast\", \"fox\", \"runs\", \"past\", \"the\", \"sleepy\", \"cat\"],\n",
    "    [\"the\", \"dog\", \"barks\", \"at\", \"the\", \"noisy\", \"crowd\"],\n",
    "    [\"cats\", \"and\", \"dogs\", \"live\", \"together\", \"in\", \"peace\"],\n",
    "    [\"the\", \"quick\", \"hare\", \"outruns\", \"the\", \"slow\", \"tortoise\"],\n",
    "    [\"brown\", \"foxes\", \"are\", \"common\", \"in\", \"this\", \"area\"],\n",
    "    [\"lazy\", \"animals\", \"sleep\", \"all\", \"day\", \"long\"],\n",
    "    [\"the\", \"noisy\", \"crowd\", \"disrupted\", \"the\", \"quiet\", \"park\"]\n",
    "] *10\n",
    "\n",
    "vocabulary = sorted(list(set([word for sentence in corpus for word in sentence])))\n",
    "print(f\"Vocabulary size: {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2541bd",
   "metadata": {},
   "source": [
    "Step 2: Initialize and train Word2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "context_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25710696",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(V=vocabulary, d=embedding_dim, M=context_size, skipgram=False)\n",
    "skipgram_model = Word2Vec(V=vocabulary, d=embedding_dim, M=context_size, skipgram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_embeddings = cbow_model.embed(\n",
    "    corpus,\n",
    "    alpha=0.05,\n",
    "    epochs=100,\n",
    "    decay=\"exp\",\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "skipgram_embeddings = skipgram_model.embed(\n",
    "    corpus,\n",
    "    alpha=0.05,\n",
    "    epochs=100,\n",
    "    decay=\"exp\",\n",
    "    decay_rate=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c3c2c",
   "metadata": {},
   "source": [
    "Step 3: Analyze Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(model, embeddings, name):\n",
    "    print(f\"\\n===== {name} Model Analysis =====\")\n",
    "    \n",
    "    test_words = [\"fox\", \"dog\", \"quick\", \"lazy\", \"noisy\"]\n",
    "    for word in test_words:\n",
    "        if word not in model.word_ind_map:\n",
    "            continue\n",
    "            \n",
    "        word_idx = model.word_ind_map[word]\n",
    "        word_vec = embeddings[word_idx].numpy().reshape(1, -1)\n",
    "        \n",
    "        all_vecs = embeddings.numpy()\n",
    "        similarities = cosine_similarity(word_vec, all_vecs)[0]\n",
    "\n",
    "        top_indices = np.argsort(similarities)[-6:-1][::-1]\n",
    "        similar_words = [(vocabulary[i], similarities[i]) for i in top_indices]\n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        for w, sim in similar_words:\n",
    "            print(f\"{w}: {sim:.3f}\")\n",
    "\n",
    "    def plot_embeddings(embeddings, title):\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        tsne = TSNE(n_components=2, perplexity=5)\n",
    "        \n",
    "        pca_results = pca.fit_transform(embeddings)\n",
    "        tsne_results = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(pca_results[:, 0], pca_results[:, 1])\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            plt.annotate(word, xy=(pca_results[i, 0], pca_results[i, 1]))\n",
    "        plt.title(f\"{title} - PCA\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            plt.annotate(word, xy=(tsne_results[i, 0], tsne_results[i, 1]))\n",
    "        plt.title(f\"{title} - t-SNE\")\n",
    "        plt.show()\n",
    "    \n",
    "    plot_embeddings(embeddings.numpy(), name)\n",
    "\n",
    "    def test_analogy(w1, w2, w3):\n",
    "        try:\n",
    "            vec = embeddings[model.word_ind_map[w2]] - embeddings[model.word_ind_map[w1]] + embeddings[model.word_ind_map[w3]]\n",
    "            similarities = cosine_similarity(vec.reshape(1, -1), embeddings.numpy())[0]\n",
    "            most_similar = vocabulary[np.argmax(similarities)]\n",
    "            print(f\"{w1} : {w2} :: {w3} : {most_similar}\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\nAnalogy tests:\")\n",
    "    test_analogy(\"quick\", \"fox\", \"dog\")\n",
    "    test_analogy(\"lazy\", \"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_embeddings(cbow_model, cbow_embeddings, \"CBOW\")\n",
    "analyze_embeddings(skipgram_model, skipgram_embeddings, \"SkipGram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391429d",
   "metadata": {},
   "source": [
    "Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, embeddings):\n",
    "    synonym_pairs = [(\"quick\", \"fast\"), (\"lazy\", \"sleepy\"), (\"noisy\", \"loud\")]\n",
    "    unrelated_pairs = [(\"fox\", \"cat\"), (\"quick\", \"sleepy\"), (\"dog\", \"park\")]\n",
    "    \n",
    "    synonym_sims, unrelated_sims = [], []\n",
    "    \n",
    "    for w1, w2 in synonym_pairs:\n",
    "        if w1 in model.word_ind_map and w2 in model.word_ind_map:\n",
    "            vec1 = embeddings[model.word_ind_map[w1]]\n",
    "            vec2 = embeddings[model.word_ind_map[w2]]\n",
    "            synonym_sims.append(torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2)))\n",
    "    \n",
    "    for w1, w2 in unrelated_pairs:\n",
    "        if w1 in model.word_ind_map and w2 in model.word_ind_map:\n",
    "            vec1 = embeddings[model.word_ind_map[w1]]\n",
    "            vec2 = embeddings[model.word_ind_map[w2]]\n",
    "            unrelated_sims.append(torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2)))\n",
    "    \n",
    "    print(f\"\\nMean similarity for synonyms: {torch.mean(torch.tensor(synonym_sims)):.3f}\")\n",
    "    print(f\"Mean similarity for unrelated: {torch.mean(torch.tensor(unrelated_sims)):.3f}\")\n",
    "    print(f\"Discrimination ratio: {torch.mean(torch.tensor(synonym_sims)) / torch.mean(torch.tensor(unrelated_sims)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2414a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== CBOW Evaluation ===\")\n",
    "evaluate_model(cbow_model, cbow_embeddings)\n",
    "\n",
    "print(\"\\n=== SkipGram Evaluation ===\")\n",
    "evaluate_model(skipgram_model, skipgram_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
